{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T10:24:23.036941Z",
     "start_time": "2022-04-06T10:24:22.957011Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import sleep \n",
    "import validators\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import date\n",
    "import shutil\n",
    "import logging\n",
    "import pathlib\n",
    "import tarfile\n",
    "import lzma\n",
    "from natsort import natsorted\n",
    "\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dateutil.parser import parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T10:24:23.092835Z",
     "start_time": "2022-04-06T10:24:23.038015Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('../synbiobot_CORE'):\n",
    "    sys.path.append('../synbiobot_CORE')\n",
    "\n",
    "from airtable_config import *\n",
    "from benchling_tools import benchling_to_gb, get_benchling_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upload_download_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_handler = logging.StreamHandler()  # Console handler\n",
    "file_handler = logging.FileHandler('log.log')  # File handler\n",
    "\n",
    "# Configure the logging module\n",
    "logging.basicConfig(level=logging.INFO,  # Set logging level to INFO\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # Set log message format\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',  # Set date format for log messages\n",
    "                    handlers=[console_handler, file_handler])  # Log to both console and file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T10:24:31.690482Z",
     "start_time": "2022-04-06T10:24:23.317852Z"
    }
   },
   "outputs": [],
   "source": [
    "# synbio airtable credentials\n",
    "api_key = os.getenv(\"AIRTABLE_API_KEY\")\n",
    "C_table_id = os.getenv(\"TABLE_C_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_C_table= partial(get_table, C_table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_size_in_bytes(file_path):\n",
    "    size = os.path.getsize(file_path)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_folder_to_tar_xz(folder_path, output_file):\n",
    "    with lzma.open(output_file, \"w\") as f:\n",
    "        with tarfile.open(fileobj=f, mode=\"w\") as tar:\n",
    "            for root, dirs, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    tar.add(file_path, arcname=os.path.relpath(file_path, folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_tar_xz_files(directory):\n",
    "    return [file for file in os.listdir(directory) if file.endswith(\".tar.xz\")]\n",
    "\n",
    "def get_datetime_from_filename(filename):\n",
    "    timestamp_str = filename.split('_')[1].split('.')[0]\n",
    "    return parse(timestamp_str)\n",
    "\n",
    "def get_recent_archives(directory):\n",
    "    files = list_tar_xz_files(directory)\n",
    "    sorted_files = sorted(files, key=get_datetime_from_filename, reverse=True)\n",
    "\n",
    "    last_year = datetime.datetime.now() - relativedelta(years=1)\n",
    "    last_12_months = [datetime.datetime.now() - relativedelta(months=i) for i in range(1, 13)]\n",
    "    last_4_weeks = [datetime.datetime.now() - relativedelta(weeks=i) for i in range(1, 5)]\n",
    "    last_7_days = [datetime.datetime.now() - datetime.timedelta(days=i) for i in range(1, 8)]\n",
    "    today = datetime.datetime.now().date()\n",
    "\n",
    "    def get_most_recent(files, dates):\n",
    "        recent_files = []\n",
    "        for date in dates:\n",
    "            recent_file = None\n",
    "            closest_time_diff = float('inf')\n",
    "            for file in files:\n",
    "                file_date = get_datetime_from_filename(file)\n",
    "                time_diff = (date - file_date).total_seconds()\n",
    "                if 0 <= time_diff < closest_time_diff:\n",
    "                    closest_time_diff = time_diff\n",
    "                    recent_file = file\n",
    "            if recent_file:\n",
    "                recent_files.append(recent_file)\n",
    "        return recent_files\n",
    "\n",
    "    # Check for today's backups\n",
    "    todays_backups = [file for file in files if get_datetime_from_filename(file).date() == today]\n",
    "\n",
    "    keep_archives = list()\n",
    "    keep_archives.extend(todays_backups)\n",
    "    keep_archives.extend(get_most_recent(sorted_files, [last_year]))\n",
    "    keep_archives.extend(get_most_recent(sorted_files, last_12_months))\n",
    "    keep_archives.extend(get_most_recent(sorted_files, last_4_weeks))\n",
    "    keep_archives.extend(get_most_recent(sorted_files, last_7_days))\n",
    "\n",
    "    return keep_archives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T10:25:46.698505Z",
     "start_time": "2022-04-06T10:25:36.182278Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    # prepare folder\n",
    "    try:\n",
    "        now = str(datetime.datetime.now())\n",
    "    \n",
    "        backup_folder_main = pathlib.Path(\"../benchling_backups\")\n",
    "        backup_folder = backup_folder_main/f\"benchling_{now}\"\n",
    "        \n",
    "        if not os.path.exists(backup_folder):\n",
    "            os.makedirs(backup_folder)\n",
    "        \n",
    "        C_table = get_C_table()\n",
    "        C_records = C_table.all()\n",
    "    except:\n",
    "        logging.exception(\"Initialisation failed\")\n",
    "        sleep(60)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # download all constructs\n",
    "    unsaved=list()\n",
    "    for C_record in C_records:\n",
    "    \n",
    "        sleep(0.5)\n",
    "    \n",
    "        url=None\n",
    "    \n",
    "        C_ID = C_record.get(\"fields\").get(\"ID\")\n",
    "        url=C_record.get(\"fields\").get(\"Benchling link (public)\",\"\")\n",
    "    \n",
    "        saved=False\n",
    "        if validators.url(url):\n",
    "    \n",
    "            # get json if possible\n",
    "            try:\n",
    "                get_benchling_json(url, backup_folder/f\"{C_ID}.json\")\n",
    "                logging.info(f\"Saved {C_ID} as json\")\n",
    "                saved=True\n",
    "            except:\n",
    "                logging.exception(f\"Failed to save {C_ID} as json\")\n",
    "    \n",
    "            # attempt to get the gb file\n",
    "            try:\n",
    "                benchling_to_gb(url, backup_folder/f\"{C_ID}.gb\")\n",
    "                logging.info(f\"Saved {C_ID} as gb\")\n",
    "                saved=True\n",
    "            except:\n",
    "                logging.exception(f\"Failed to save{C_ID} as gb.\")\n",
    "    \n",
    "        else:\n",
    "            logging.info(f\"Construct {C_ID} does not have a valid URL\")\n",
    "    \n",
    "        if not saved:\n",
    "            unsaved.append(str(C_ID))\n",
    "    \n",
    "    with open(backup_folder/\"unsaved.txt\",\"w\") as out:\n",
    "        out.write(\"\\n\".join(unsaved))\n",
    "    \n",
    "    \n",
    "    # compress in archive, then remove downloaded files\n",
    "    try:\n",
    "        compress_folder_to_tar_xz(backup_folder, f\"{str(backup_folder)}.tar.xz\")\n",
    "        shutil.rmtree(backup_folder)\n",
    "    except:\n",
    "        logging.exception(\"Failed to compress backup folder\")\n",
    "\n",
    "\n",
    "    # upload to google drive\n",
    "    try:\n",
    "        upload_to_drive(remote_name, f\"{str(backup_folder)}.tar.xz\")\n",
    "    except:\n",
    "        logging.exception(\"Failed to backup snapshot to google drive.\")\n",
    "\n",
    "    # keep the...\n",
    "    # most recent backup from one year ago\n",
    "    # most recent backup from each of the last 12 months\n",
    "    # most recent backup from each of the last 4 weeks\n",
    "    # most recent backup from each of the last 7 days\n",
    "    # any backup created today\n",
    "    # remove the rest\n",
    "\n",
    "    keep_archives.extend(todays_backups)\n",
    "    keep_archives.extend(get_most_recent(sorted_files, [last_year]))\n",
    "    keep_archives.extend(get_most_recent(sorted_files, last_12_months))\n",
    "    keep_archives.extend(get_most_recent(sorted_files, last_4_weeks))\n",
    "    keep_archives.extend(get_most_recent(sorted_files, last_7_days))\n",
    "    \n",
    "    all_backups = list_tar_xz_files(backup_folder_main)\n",
    "    keep_backups = get_recent_archives(backup_folder_main)\n",
    "\n",
    "    for backup in all_backups:\n",
    "        if backup not in keep_backups:\n",
    "            os.remove(backup)\n",
    "\n",
    "    sleep(86400) # wait one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "./synbiobot_backup_benchling/",
   "language": "python",
   "name": "synbiobot_backup_benchling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "",
   "toc_cell": false,
   "toc_position": {
    "height": "713.636px",
    "left": "21px",
    "top": "98.9631px",
    "width": "375.781px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
